# -*- coding: utf-8 -*-
"""TP53_AI_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ous_90EqMmCU474vMeedxobljvifwvaq
"""

import os
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

DATA_RAW = "/content/drive/MyDrive/elephant-p53-project/data/raw"
OUTPUT_PATH = "/content/drive/MyDrive/elephant-p53-project/data/processed/TP53_all_sequences.fasta"

# List of input FASTA files
input_files = [
    "human_tp53.fasta",
    "asian_tp53_hits.fasta",
    "african_tp53_hits.fasta"
]

# Merge files
with open(OUTPUT_PATH, "w") as outfile:
    for f in input_files:
        file_path = os.path.join(DATA_RAW, f)
        if os.path.exists(file_path):
            with open(file_path, "r") as infile:
                outfile.write(infile.read().strip() + "\n")
        else:
            print(f"âš  File not found: {f}")

print("âœ” Combined FASTA created at:", OUTPUT_PATH)

!pip install biopython
!apt-get install muscle

from Bio import SeqIO

input_file = "/content/drive/MyDrive/elephant-p53-project/data/processed/TP53_all_sequences.fasta"
clean_file = "/content/drive/MyDrive/elephant-p53-project/data/processed/TP53_clean.fasta"

seen_ids = set()
cleaned_records = []

for record in SeqIO.parse(input_file, "fasta"):
    clean_id = record.id.replace("|", "_").split()[0]
    if clean_id not in seen_ids:
        record.id = clean_id
        record.description = ""
        cleaned_records.append(record)
        seen_ids.add(clean_id)

SeqIO.write(cleaned_records, clean_file, "fasta")

print(f"âœ” Cleaned FASTA created: {clean_file}")
print(f"Total sequences retained: {len(cleaned_records)}")

# Cleaned input FASTA from previous cell
clean_file = "/content/drive/MyDrive/elephant-p53-project/data/processed/TP53_clean.fasta"

# Output MSA file path
msa_output = "/content/drive/MyDrive/elephant-p53-project/results/msa/TP53_MSA.fasta"

# Run MUSCLE MSA (v3 syntax)
!muscle -in {clean_file} -out {msa_output}

print("ðŸŽ¯ MSA Completed Successfully!")
print("MSA saved to:", msa_output)

import os

msa_output = "/content/drive/MyDrive/TP53_Project/results/msa/TP53_MSA.fasta"
print("Exists?", os.path.exists(msa_output))

!pip install biopython logomaker

from Bio import AlignIO
import pandas as pd
import logomaker
import matplotlib.pyplot as plt

msa_output = "/content/drive/MyDrive/elephant-p53-project/results/msa/TP53_MSA.fasta"

# Load MSA file
alignment = AlignIO.read(msa_output, "fasta")

# Convert MSA to alignment matrix
AlignmentArray = []
for record in alignment:
    AlignmentArray.append(list(str(record.seq)))

df = pd.DataFrame(AlignmentArray).T  # transpose â†’ positions in rows

# Convert to probability matrix (PPM)
ppm = df.apply(lambda x: x.value_counts(normalize=True), axis=1).fillna(0)

# Remove gap column if exists
if "-" in ppm.columns:
    ppm = ppm.drop(columns="-")

# Plot sequence logo
plt.figure(figsize=(25, 6))
logomaker.Logo(ppm, shade_below=0.2, fade_below=0.1)
plt.title("TP53 Elephant + Human MSA - Sequence Logo")
plt.xlabel("Protein Sequence Position")
plt.ylabel("Frequency")
plt.tight_layout()

# Save figure
fig_path = "/content/drive/MyDrive/elephant-p53-project/results/msa/TP53_MSA_logo.png"
plt.savefig(fig_path, dpi=300)
print("Saved:", fig_path)

from Bio import SeqIO
import pandas as pd

clean_fasta = "/content/drive/MyDrive/elephant-p53-project/data/processed/TP53_clean.fasta"

records = list(SeqIO.parse(clean_fasta, "fasta"))

rows = []
for rec in records:
    seq = str(rec.seq)
    length = len(seq)
    aa_counts = {aa: seq.count(aa) for aa in "ACDEFGHIKLMNPQRSTVWY"}
    gc_like = seq.count("G") + seq.count("C")

    row = {
        "id": rec.id,
        "length": length,
        "GC_like": gc_like,
    }
    # Add AA composition (fraction)
    for aa, count in aa_counts.items():
        row[f"frac_{aa}"] = count / length

    rows.append(row)

df = pd.DataFrame(rows)
basic_features_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/basic_features.csv"
df.to_csv(basic_features_path, index=False)

print("âœ” Saved:", basic_features_path)
df.head()

from Bio import pairwise2

clean_fasta = "/content/drive/MyDrive/elephant-p53-project/data/processed/TP53_clean.fasta"
records = list(SeqIO.parse(clean_fasta, "fasta"))

# Identify human TP53 sequence
human = None
for rec in records:
    if "P53_HUMAN" in rec.id or "human" in rec.id.lower():
        human = rec
        break

if human is None:
    print("âš  Could not auto-detect human TP53. Check ID in FASTA header.")
else:
    print("Using human TP53 id:", human.id)

# Load previous features
import pandas as pd
basic_features_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/basic_features.csv"
df = pd.read_csv(basic_features_path)

similarities = []
for rec in records:
    align = pairwise2.align.globalxx(human.seq, rec.seq, one_alignment_only=True, score_only=True)
    max_score = min(len(human.seq), len(rec.seq))
    identity = align / max_score
    similarities.append({"id": rec.id, "identity_to_human": round(identity * 100, 2)})

sim_df = pd.DataFrame(similarities)

df = df.merge(sim_df, on="id", how="left")

features_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/tp53_features_with_similarity.csv"
df.to_csv(features_path, index=False)

print("âœ” Saved:", features_path)
df.head()

import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans  # âœ… correct import

# Path to the feature file from previous steps
features_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/tp53_features_with_similarity.csv"

# Load the features
df = pd.read_csv(features_path)
print("Columns in feature table:")
print(df.columns.tolist())

# Make sure required columns exist
if "identity_to_human" not in df.columns:
    raise ValueError("Column 'identity_to_human' not found. "
                     "Please re-run the similarity cell to create it.")

# Keep ID separate
id_col = "id"
if id_col not in df.columns:
    raise ValueError("Column 'id' not found in the features file.")

# Select ONLY numeric columns for clustering
numeric_df = df.select_dtypes(include=["int64", "float64"])
print("\nNumeric columns used for clustering:")
print(numeric_df.columns.tolist())

# Scale numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(numeric_df)

# KMeans clustering into 3 groups
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_scaled)

# Attach cluster labels back to original df
df["cluster"] = clusters

# Sort by similarity to human TP53 (descending)
df_sorted = df.sort_values("identity_to_human", ascending=False)

print("\nTop 10 sequences by similarity to human TP53:")
display(df_sorted[["id", "identity_to_human", "cluster"]].head(10))

# Plot top 20 as bar plot
top_df = df_sorted.head(20)

plt.figure(figsize=(12, 6))
plt.bar(top_df["id"], top_df["identity_to_human"])
plt.xticks(rotation=90)
plt.ylabel("% Identity to Human TP53")
plt.title("Top TP53/RTG sequences by similarity to human p53")
plt.tight_layout()

plot_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/identity_barplot.png"
plt.savefig(plot_path, dpi=300)
print("\nâœ… Barplot saved to:", plot_path)

df.columns

import pandas as pd

features_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/tp53_features_with_similarity.csv"
df = pd.read_csv(features_path)

# Sort by similarity
df_sorted = df.sort_values("identity_to_human", ascending=False)

# Show top 10 (id + similarity)
df_sorted[["id", "identity_to_human"]].head(10)

top10 = df_sorted[["id", "identity_to_human"]].head(10)
top10_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/top10_tp53_like.csv"
top10.to_csv(top10_path, index=False)
print("Saved top 10 to:", top10_path)

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

features_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/tp53_features_with_similarity.csv"
df = pd.read_csv(features_path)

# Keep ID separately
id_col = "id"

# Select only numeric features
numeric_df = df.select_dtypes(include=["int64", "float64"])

# Scale numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(numeric_df)

# Run k-means with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
df["cluster"] = kmeans.fit_predict(X_scaled)

# Sort by similarity and show top 10 with cluster
df_sorted = df.sort_values("identity_to_human", ascending=False)
df_sorted[["id", "identity_to_human", "cluster"]].head(10)

features_with_cluster_path = "/content/drive/MyDrive/elephant-p53-project/results/ML/tp53_features_with_similarity_clustered.csv"
df_sorted.to_csv(features_with_cluster_path, index=False)
print("Saved:", features_with_cluster_path)